import re
import time
from urllib.parse import urljoin
from bs4 import BeautifulSoup
import requests
import os

# Supabase Client'Ä±n TanÄ±mlandÄ±ÄŸÄ± VarsayÄ±mÄ±
try:
    from scripts.shared.supabase_client import supabase
except ImportError:
    print("âš ï¸ Supabase client import edilemedi. VeritabanÄ± iÅŸlemleri pas geÃ§ilecektir.")
    supabase = None 

BASE_URL = "https://www.oksid.com.tr"

# DÃ¼z Requests iÃ§in BaÅŸlÄ±klar
HEADERS = {
    'User-Agent': (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    ),
    'Accept-Language': 'tr-TR,tr;q=0.9,en-US;q=0.8,en;q=0.7',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Connection': 'keep-alive',
}

# Proxy YapÄ±landÄ±rmasÄ±
PROXY_URL = os.getenv('TR_PROXY_URL')
PROXIES = {}
if PROXY_URL:
    PROXIES = {
        'http': PROXY_URL,
        'https': PROXY_URL,
    }

# --- HTML Ã‡ekme Fonksiyonu ---
def fetch_html(url, retries=3, backoff=5): 
    for attempt in range(retries):
        try:
            res = requests.get(url, headers=HEADERS, proxies=PROXIES, timeout=120) 
            res.raise_for_status()
            if res.history:
                 print(f"âš ï¸ YÃ¶nlendirme Tespit Edildi! Son URL: {res.url}")
            return BeautifulSoup(res.text, "html.parser")
        except Exception as e:
            print(f"âš ï¸ Hata {e} (URL: {url}) â†’ retry {attempt+1}/{retries}")
            if attempt < retries - 1:
                time.sleep(backoff * (attempt + 1))
            else:
                raise

# --- Fiyat Temizleme Fonksiyonu ---
def clean_price(price_text):
    if not price_text:
        return None
    cleaned_text = re.sub(r"[^\d,.]", "", price_text)
    if "," in cleaned_text and cleaned_text.count(",") == 1:
        cleaned_text = cleaned_text.replace(".", "").replace(",", ".")
    elif "." in cleaned_text and cleaned_text.count(".") > 1:
        parts = cleaned_text.split(".")
        cleaned_text = "".join(parts[:-1]) + "." + parts[-1]
    try:
        return float(cleaned_text)
    except:
        return None

# --- GÃœNCELLENMÄ°Å SUPABASE KAYDETME FONKSÄ°YONU (PRICE_1 KONTROLÃœ VE STOK KONTROLÃœ) ---
def save_to_supabase(products, category_name, batch_size=50):
    """
    Sadece yeni veya price_1/stok durumu deÄŸiÅŸen Ã¼rÃ¼nleri DB'ye yazar.
    Eski price_1 deÄŸerini last_price'a kaydeder.
    """
    if not products or not supabase:
        print("âŒ Supabase client eksik veya Ã¼rÃ¼n listesi boÅŸ. KayÄ±t atlandÄ±.")
        return

    product_urls = [p["url"] for p in products if p.get("url")]
    existing_products = {}
    
    # AdÄ±m 1: Mevcut Ã¼rÃ¼nleri DB'den verimli ÅŸekilde Ã§ek
    SELECT_CHUNK_SIZE = 900 
    if product_urls:
        print(f"ğŸ“Š DB'den {len(product_urls)} Ã¼rÃ¼nÃ¼n mevcut durumu sorgulanacak...")
        for i in range(0, len(product_urls), SELECT_CHUNK_SIZE):
            url_chunk = product_urls[i:i + SELECT_CHUNK_SIZE]
            try:
                # DEÄÄ°ÅÄ°KLÄ°K: VeritabanÄ±ndan artÄ±k 'price_1' bilgisini Ã§ekiyoruz.
                response = supabase.table("oksid_products").select("url, price_1, stock").in_("url", url_chunk).execute()
                for item in response.data:
                    existing_products[item["url"]] = item
                print(f"   -> {len(response.data)} mevcut Ã¼rÃ¼n bilgisi alÄ±ndÄ± (grup {i//SELECT_CHUNK_SIZE + 1}).")
            except Exception as e:
                print(f"âš ï¸ Mevcut Ã¼rÃ¼nler Ã§ekilirken hata (grup {i//SELECT_CHUNK_SIZE + 1}): {e}")
        print(f"âœ… Toplam {len(existing_products)} mevcut Ã¼rÃ¼n bilgisi baÅŸarÄ±yla alÄ±ndÄ±.")

    # AdÄ±m 2: Sadece gÃ¼ncellenecek veya eklenecek Ã¼rÃ¼nleri belirle
    products_to_upsert = []
    print("\nğŸ” DeÄŸiÅŸiklikler kontrol ediliyor...")
    for p in products:
        url = p.get("url")
        if not url:
            continue

        # Durum 1: Yeni Ã¼rÃ¼n
        if url not in existing_products:
            print(f"âœ¨ Yeni Ã¼rÃ¼n bulundu: {p['name'][:60]}...")
            p['last_price'] = None
            products_to_upsert.append(p)
            continue
        
        # KarÅŸÄ±laÅŸtÄ±rma iÃ§in mevcut Ã¼rÃ¼n bilgilerini al
        existing_product = existing_products[url]
        
        # DEÄÄ°ÅÄ°KLÄ°K: FiyatlarÄ± 'price_1' Ã¼zerinden alÄ±yoruz.
        old_price = existing_product.get("price_1")
        new_price = p.get("price_1")
        
        # Stok durumlarÄ±nÄ± al
        old_stock = existing_product.get("stock")
        new_stock = p.get("stock")

        # DeÄŸiÅŸiklik olup olmadÄ±ÄŸÄ±nÄ± kontrol et
        price_has_changed = (old_price is not None and new_price is not None and old_price != new_price)
        stock_has_changed = (old_stock is not None and new_stock is not None and old_stock != new_stock)

        # EÄŸer fiyat VEYA stok durumu deÄŸiÅŸmiÅŸse, Ã¼rÃ¼nÃ¼ gÃ¼ncelleme listesine ekle
        if price_has_changed or stock_has_changed:
            change_reasons = []
            
            if price_has_changed:
                # DEÄÄ°ÅÄ°KLÄ°K: 'last_price' alanÄ±na eski 'price_1' deÄŸerini atÄ±yoruz.
                p['last_price'] = old_price
                change_reasons.append(f"Fiyat: {old_price} -> {new_price}")
            else:
                p['last_price'] = old_price
            
            if stock_has_changed:
                change_reasons.append(f"Stok: '{old_stock}' -> '{new_stock}'")

            log_message = " | ".join(change_reasons)
            print(f"ğŸ”„ GÃ¼ncelleme: {p['name'][:50]}... | {log_message}")
            
            products_to_upsert.append(p)

    # AdÄ±m 3: Sadece filtrelenmiÅŸ listeyi veritabanÄ±na yaz
    if not products_to_upsert:
        print(f"\nâœ… VeritabanÄ± gÃ¼ncel. '{category_name}' kategorisinde deÄŸiÅŸiklik veya yeni Ã¼rÃ¼n bulunamadÄ±.")
        return
        
    print(f"\nğŸ’¾ '{category_name}' kategorisinde {len(products_to_upsert)} Ã¼rÃ¼nde deÄŸiÅŸiklik tespit edildi. VeritabanÄ± gÃ¼ncelleniyor...")
    for p in products_to_upsert:
        p["marketplace"] = "oksid"
        p["created_at"] = time.strftime("%Y-%m-%d %H:%M:%S")
        p["last_updated"] = time.strftime("%Y-%m-%d %H:%M:%S")

    for i in range(0, len(products_to_upsert), batch_size):
        chunk = products_to_upsert[i:i+batch_size]
        for attempt in range(3):
            try:
                data = (
                    supabase.table("oksid_products")
                    .upsert(chunk, on_conflict="url")
                    .execute()
                )
                print(f"âœ… DB'ye {len(data.data)} Ã¼rÃ¼n yazÄ±ldÄ± (chunk {i//batch_size+1})")
                break
            except Exception as e:
                print(f"âš ï¸ Supabase error (chunk {i//batch_size+1}), retry {attempt+1}/3: {e}")
                time.sleep(5)

# --- ÃœrÃ¼n SayfasÄ± Tarama Fonksiyonu ---
def crawl_product_page(url, category_name):
    products = []
    page_num = 1
    while True:
        print(f"ğŸ“„ '{category_name}' kategorisi, sayfa {page_num} taranÄ±yor...")
        soup = fetch_html(url)

        if not soup:
            break

        product_list_div = soup.select_one("div.colProductIn.shwstock.shwcheck.colPrdList")
        if not product_list_div or not product_list_div.select("ul li"):
            print(f"ğŸ '{category_name}' kategorisi iÃ§in son sayfaya ulaÅŸÄ±ldÄ±.")
            break

        for li in product_list_div.select("ul li"):
             try:
                 link_tag = li.select_one("a.ihlog.product_click")
                 if not link_tag: continue
 
                 name = link_tag.get("data-name", "N/A")
                 url_product = urljoin(BASE_URL, link_tag.get("href", ""))
 
                 price1, price2, currency = None, None, None
                 p1_tag = li.select_one("span.fiyat1")
                 if p1_tag:
                     price1_text = p1_tag.get_text(strip=True)
                     price1 = clean_price(price1_text)
                     cur = re.search(r"[â‚º$â‚¬]", price1_text)
                     if cur: currency = cur.group(0)
 
                 p2_tag = li.select_one("span.fiyat3")
                 if p2_tag: price2 = clean_price(p2_tag.get_text(strip=True))
 
                 stock_span = li.select_one("span.stock")
                 stock = "Bilinmiyor"
                 if stock_span:
                     classes = stock_span.get("class", [])
                     if "stocktel" in classes: stock = "Stokta Yok"
                     elif any(re.match(r"^stock\d+$", c) for c in classes): stock = "Stokta Var"
 
                 products.append({
                     "name": name, "url": url_product, "price_1": price1,
                     "price_2": price2, "currency": currency, "stock": stock,
                     "category": category_name,
                 })
             except Exception:
                 continue

        next_page = soup.select_one("a.next")
        if not next_page: break
        href = next_page.get("href")
        if not href or href.startswith("javascript"): break
        
        url = urljoin(BASE_URL, href)
        page_num += 1
        time.sleep(1)

    if products:
        print(f"ğŸ’¾ '{category_name}' kategorisinden {len(products)} Ã¼rÃ¼n Ã§ekildi. VeritabanÄ± kontrol ediliyor...")
        save_to_supabase(products, category_name)

# --- Kategori Tarama Fonksiyonu ---
def crawl_category(url, category_name="Ana Sayfa"):
    print(f"\nğŸ“‚ Kategori baÅŸlÄ±yor: {category_name} â†’ {url}") 
    try:
        crawl_product_page(url, category_name)
    except Exception as e:
        print(f"âŒ {category_name} genel hatasÄ±: {e}")
    time.sleep(2)

# --- Kategori Tarama Fonksiyonu (ArtÄ±k Gerekli DeÄŸil, Silebilirsiniz) ---
# def crawl_category(url, category_name="Ana Sayfa"): ...

# --- GÃœNCELLENMÄ°Å Ana Fonksiyon (HÄ°YERARÅÄ°K TARAMA) ---
def crawl_from_homepage():
    print("ğŸš€ Oksid Scraper baÅŸlÄ±yor...")
    
    if PROXIES:
        proxy_host = PROXY_URL.split('@')[-1] if '@' in PROXY_URL else PROXY_URL
        print(f"âœ… Proxy ile Ã§alÄ±ÅŸÄ±lÄ±yor: {proxy_host}")
    else:
        print("â„¹ï¸ Proxy ayarÄ± bulunamadÄ±. Direkt baÄŸlantÄ± kullanÄ±lacak.")

    try:
        soup = fetch_html(BASE_URL)
        if not soup:
            print("âŒ Ana sayfa Ã§ekilemedi. Tarama durduruldu.")
            return
    except Exception as e:
        print(f"âŒ Ana sayfa hatasÄ±: {e}. Tarama durduruldu.")
        return

    # SADECE ana kategori list item'larÄ±nÄ± seÃ§iyoruz (iÃ§ iÃ§e olanlarÄ± deÄŸil)
    # '>' iÅŸareti, sadece doÄŸrudan alt elemanlarÄ± seÃ§memizi saÄŸlar.
    main_category_lis = soup.select("div.catsMenu > ul.hidden-xs > li")
    if not main_category_lis:
        print("âš ï¸ Ana kategoriler bulunamadÄ±.")
        return

    print(f"ğŸ” {len(main_category_lis)} ana kategori baÅŸlÄ±ÄŸÄ± bulundu. Ä°ÅŸlem baÅŸlÄ±yor...")

    # Her bir ana kategori baÅŸlÄ±ÄŸÄ± iÃ§in dÃ¶ngÃ¼ baÅŸlat
    for main_li in main_category_lis:
        # Ana kategorinin link etiketini ve adÄ±nÄ± al (sadece loglama iÃ§in)
        main_cat_tag = main_li.find('a', recursive=False)
        if not main_cat_tag:
            continue
        
        main_cat_name = main_cat_tag.get_text(strip=True)
        
        # EÄŸer bu ana kategorinin altÄ±nda alt menÃ¼ linkleri yoksa veya istenmeyen bir baÅŸlÄ±k ise atla
        if not main_li.select('.catSubMenu a') or main_cat_name in ["Outlet"]:
            print(f"\nâšªï¸ Ana baÅŸlÄ±k '{main_cat_name}' atlanÄ±yor (Taranacak alt kategori yok).")
            continue

        print(f"\nğŸ“ Ana Kategori TaranÄ±yor: {main_cat_name}")
        time.sleep(1) # LoglarÄ±n okunabilirliÄŸi iÃ§in kÄ±sa bekleme

        # Bu ana kategorinin iÃ§indeki TÃœM alt kategori linklerini bul
        sub_cat_tags = main_li.select('.catSubMenu a')
        
        # Alt kategorileri de kendi iÃ§inde tekilleÅŸtir (Ã¶rn: resimli ve metin linkleri)
        unique_sub_cats = {}
        for sub_a in sub_cat_tags:
            sub_name = sub_a.get_text(strip=True)
            sub_link = sub_a.get('href')
            
            if sub_name and sub_link and sub_name != "TÃ¼m Alt Kategoriler":
                full_link = urljoin(BASE_URL, sub_link)
                # Linki anahtar olarak kullanarak aynÄ± linke sahip tekrar edenleri engelle
                unique_sub_cats[full_link] = sub_name
        
        if not unique_sub_cats:
            print(f"   -> '{main_cat_name}' iÃ§inde taranacak alt kategori bulunamadÄ±.")
            continue

        # TekilleÅŸtirilmiÅŸ alt kategorileri tara
        for link, name in unique_sub_cats.items():
            print(f"  â¡ï¸ Alt Kategori BaÅŸlatÄ±lÄ±yor: {name}")
            try:
                # ÃœrÃ¼nleri doÄŸrudan alt kategori adÄ± ile kaydetmek iÃ§in crawl_product_page Ã§aÄŸÄ±r
                crawl_product_page(link, name)
            except Exception as e:
                print(f"  âŒ '{name}' alt kategorisinde genel hata: {e}")
            time.sleep(2) # Alt kategoriler arasÄ± bekleme
    
    print("\nâœ… TÃ¼m kategoriler tamamlandÄ±.")

if __name__ == "__main__":
    crawl_from_homepage()