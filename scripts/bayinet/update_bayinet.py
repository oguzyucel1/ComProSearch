import time
import urllib.parse
import json
import os
from urllib.parse import urljoin, urlparse, parse_qs
from playwright.sync_api import sync_playwright
# LÃ¼tfen projenizde scripts/shared/supabase_client.py dosyasÄ±nÄ±n
# doÄŸru ÅŸekilde ayarlandÄ±ÄŸÄ±ndan emin olun.
from scripts.shared.supabase_client import supabase 

# --- Sabitler ---
BASE_URL = "https://www.bayinet.com.tr/"
OTP_TABLE = "otp_codes" 

# --- Supabase OTP YÃ¶netimi ---
def get_and_clear_otp(timeout=180, poll_interval=5):
    """
    Supabase'den OTP'yi bekleyerek Ã§eker ve veritabanÄ±ndan siler (Polling).
    """
    start_time = time.time()
    
    print(f"â³ {timeout} saniye boyunca DB'den OTP bekleniyor...")

    while time.time() - start_time < timeout:
        try:
            response = supabase.table(OTP_TABLE).select("otp_code").limit(1).execute()
            
            if response.data and response.data[0] and response.data[0].get("otp_code"):
                otp_code = response.data[0]["otp_code"]
                print(f"âœ… OTP bulundu. ({int(time.time() - start_time)} saniye bekleme)")

                delete_response = supabase.table(OTP_TABLE).delete().eq("otp_code", otp_code).execute()
                print(f"ğŸ—‘ï¸ OTP veritabanÄ±ndan silindi. Silinen satÄ±r sayÄ±sÄ±: {len(delete_response.data)}")

                return otp_code
            
        except Exception as e:
            print(f"âš ï¸ Supabase OTP sorgu/silme hatasÄ±: {e}")
            
        time.sleep(poll_interval)
        
    raise TimeoutError(f"ğŸš¨ OTP {timeout} saniye iÃ§inde girilmedi, iÅŸlem iptal edildi.")

# --- Supabase ÃœrÃ¼n KaydÄ± (Fiyat GeÃ§miÅŸi ile) ---
# GÃœNCELLENDÄ°: Bu fonksiyon artÄ±k sadece yeni veya fiyatÄ± deÄŸiÅŸen Ã¼rÃ¼nleri DB'ye yazar.
def save_products_to_supabase(products, batch_size=50):
    if not products or not supabase:
        print("âŒ Supabase client eksik veya Ã¼rÃ¼n listesi boÅŸ. KayÄ±t atlandÄ±.")
        return

    product_ids = [p["product_id"] for p in products if p.get("product_id")]
    existing_products = {}
    
    # AdÄ±m 1: Mevcut Ã¼rÃ¼nlerin fiyat bilgilerini DB'den Ã§ek (bu kÄ±sÄ±m aynÄ±)
    SELECT_CHUNK_SIZE = 900 
    if product_ids:
        print(f"ğŸ“Š DB'den {len(product_ids)} Ã¼rÃ¼nÃ¼n mevcut durumu sorgulanacak...")
        for i in range(0, len(product_ids), SELECT_CHUNK_SIZE):
            id_chunk = product_ids[i:i + SELECT_CHUNK_SIZE]
            try:
                response = supabase.table("bayinet_products").select("product_id,price").in_("product_id", id_chunk).execute()
                for item in response.data:
                    existing_products[item["product_id"]] = item
                print(f"   -> {len(response.data)} mevcut Ã¼rÃ¼n bilgisi alÄ±ndÄ± (grup {i//SELECT_CHUNK_SIZE + 1}).")
            except Exception as e:
                print(f"âš ï¸ Mevcut Ã¼rÃ¼nler Ã§ekilirken hata (grup {i//SELECT_CHUNK_SIZE + 1}): {e}")
        print(f"âœ… Toplam {len(existing_products)} mevcut Ã¼rÃ¼n bilgisi baÅŸarÄ±yla alÄ±ndÄ±.")

    # AdÄ±m 2: YENÄ° - Sadece gÃ¼ncellenecek veya eklenecek Ã¼rÃ¼nleri belirle
    products_to_upsert = []
    print("\nğŸ” DeÄŸiÅŸiklikler kontrol ediliyor...")
    for p in products:
        product_id = p.get("product_id")
        if not product_id:
            continue

        # Durum 1: ÃœrÃ¼n veritabanÄ±nda yok (YENÄ° ÃœRÃœN)
        if product_id not in existing_products:
            print(f"âœ¨ Yeni Ã¼rÃ¼n bulundu: {p['name'][:60]}...")
            products_to_upsert.append(p)
            continue
        
        # Durum 2: ÃœrÃ¼n veritabanÄ±nda var, fiyatlarÄ± karÅŸÄ±laÅŸtÄ±r
        old_price = existing_products[product_id].get("price")
        new_price = p.get("price")

        # Fiyatlar farklÄ±ysa, `last_price`'Ä± ata ve gÃ¼ncelleme listesine ekle
        if old_price is not None and new_price is not None and old_price != new_price:
            p['last_price'] = old_price
            print(f"ğŸ’° Fiyat DeÄŸiÅŸti: {p['name'][:60]}... | Eski: {old_price} -> Yeni: {new_price}")
            products_to_upsert.append(p)

    # AdÄ±m 3: Sadece filtrelenmiÅŸ listeyi veritabanÄ±na yaz
    if not products_to_upsert:
        print("\nâœ… VeritabanÄ± gÃ¼ncel. DeÄŸiÅŸiklik veya yeni Ã¼rÃ¼n bulunamadÄ±.")
        return
        
    print(f"\nğŸ’¾ Toplam {len(products_to_upsert)} Ã¼rÃ¼nde deÄŸiÅŸiklik tespit edildi. VeritabanÄ± gÃ¼ncelleniyor...")
    for i in range(0, len(products_to_upsert), batch_size):
        chunk = products_to_upsert[i:i+batch_size]
        for attempt in range(3):
            try:
                data = (
                    supabase.table("bayinet_products")
                    .upsert(chunk, on_conflict="product_id")
                    .execute()
                )
                print(f"âœ… DB'ye {len(data.data)} Ã¼rÃ¼n yazÄ±ldÄ± (chunk {i//batch_size+1})")
                break
            except Exception as e:
                print(f"âš ï¸ Supabase error (chunk {i//batch_size+1}), retry {attempt+1}/3: {e}")
                time.sleep(5)

# --- Playwright ile Oturum AÃ§ma ---
def manual_login_and_get_session(p):
    CUSTOMER_CODE = os.getenv("BAYINET_CUSTOMER_CODE")
    EMAIL = os.getenv("BAYINET_EMAIL")
    PASSWORD = os.getenv("BAYINET_PASSWORD")
 
    if not all([CUSTOMER_CODE, EMAIL, PASSWORD]):
        raise RuntimeError("ğŸš¨ GiriÅŸ bilgileri eksik (env BAYINET_CUSTOMER_CODE, EMAIL, PASSWORD)!")

    launch_args = ['--no-sandbox', '--disable-setuid-sandbox', '--disable-blink-features=AutomationControlled']
    browser = p.chromium.launch(headless=True, slow_mo=50, args=launch_args)
    context = browser.new_context(
        user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36",
        viewport={"width": 1366, "height": 768}
    )
    page = context.new_page()
    page.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => false})")

    print(f"â¡ï¸ {BASE_URL}Login adresine gidiliyor...")
    page.goto(f"{BASE_URL}Login", wait_until="domcontentloaded", timeout=60000)
    page.wait_for_load_state("networkidle", timeout=30000)
    print("â¡ï¸ Login formu yÃ¼kleniyor (AÄŸ boÅŸta)...")
    
    try:
        musteri_kodu_input = page.get_by_placeholder("MÃ¼ÅŸteri Kodu").first
        musteri_kodu_input.wait_for(timeout=20000)
        print("âœ… Placeholder ile ilk input bulundu.")
        musteri_kodu_input.fill(CUSTOMER_CODE)
        page.get_by_placeholder("E-Posta veya telefon numarasÄ±").first.fill(EMAIL)
        page.get_by_placeholder("Åifre").first.fill(PASSWORD)
        print("âœ… GiriÅŸ bilgileri dolduruldu.")
    except Exception as e:
        page.screenshot(path="login_form_timeout.png")
        print(f"ğŸš¨ Login formu yÃ¼klenemedi. 'login_form_timeout.png' dosyasÄ±nÄ± kontrol edin. Hata: {e}")
        browser.close()
        raise

    page.locator("button:has-text('GiriÅŸ Yap')").click()
    page.wait_for_load_state("networkidle")
    print("â¡ï¸ GiriÅŸ yapÄ±ldÄ±, OTP ekranÄ± bekleniyor...")

    page.wait_for_selector("div.css-1phe0ka input", timeout=120000)
    print("âœ¨ OTP GiriÅŸ EkranÄ± YÃ¼klendi. DB'den kod Ã§ekiliyor.")

    try:
        otp_code = get_and_clear_otp(timeout=180, poll_interval=5)
    except TimeoutError as e:
        browser.close()
        raise e
    
    otp_inputs = page.locator("div.css-1phe0ka input")
    for i, digit in enumerate(otp_code):
        if i < otp_inputs.count():
             otp_inputs.nth(i).fill(digit)
    print("âœ… OTP kodu Playwright ile dolduruldu.")

    page.locator("button:has-text('DoÄŸrula')").click()
    page.wait_for_load_state("networkidle", timeout=60000)
    print("â¡ï¸ OTP doÄŸrulandÄ±, yÃ¶nlendirme bekleniyor...")

    print("ğŸ” Mevcut URL (OTP sonrasÄ±):", page.url)
    page.wait_for_selector("span:has-text('Bayinet')", timeout=60000)
    print("â¡ï¸ Bayinet butonu bulundu, tÄ±klanÄ±yor...")
    page.click("span:has-text('Bayinet')")

    print("â³ Bayinet ana sayfasÄ±na yÃ¶nlendiriliyor...")
    page.wait_for_selector(".menu-categories__toggle.collapsed.js_collapsed.hidden-xs", timeout=120000)
    print("âœ… Bayinet ana sayfasÄ± tamamen yÃ¼klendi. URL:", page.url)

    return page

# --- Scraper Fonksiyonu (Sadece veri Ã§eker, DB'ye yazmaz) ---
def scrape_all_pages(page, category_id, max_pages=200):
    """
    Belirli bir kategorideki tÃ¼m sayfalardaki Ã¼rÃ¼nleri Ã§eker ve bir liste olarak dÃ¶ndÃ¼rÃ¼r.
    """
    all_products_in_category = []
    for page_num in range(max_pages):
        # ... search_model ve URL oluÅŸturma kÄ±smÄ± aynÄ± ...
        search_model = {
            "SelectedProperties": [], "SelectedPropertyGroups": [], "AllProp": False, "PageNumber": page_num,
            "VisibleProductCount": "36", "SearchTextInProducts": "", "Categories": [category_id],
            "CategoriesLevel4": [], "CategoriesLevel2": [], "Brands": [], "PropertyGroups": [], "Properties": [],
            "StoragePlace": "2001", "Sorting": "en-dusuk-fiyat", "IsStockOnly": False, "AllStoragePlaces": False,
            "WithPropertiesAggregation": False, "WithPropertyGroupAggregation": False, "SearchCategoryId": category_id,
            "MaxPrice": {"Price": 0, "SelectedPrice": 0, "Currency": ""},
            "MinPrice": {"Price": 0, "SelectedPrice": 0, "Currency": ""},
            "IsSubscription": "false", "IsN11": False
        }

        url = f"{BASE_URL}Product/Index?searchModel={urllib.parse.quote(json.dumps(search_model, ensure_ascii=False))}"
        page.goto(url)
        page.wait_for_load_state("networkidle")

        product_divs = page.locator("div.product-list__item.grid-item")
        count = product_divs.count()

        if count == 0:
            print(f"â›”ï¸ Kategori {category_id}, Sayfa {page_num}: Ã¼rÃ¼n yok, bu kategori bitti.")
            break

        products_on_page = []
        for i in range(count):
            try:
                link_tag = product_divs.nth(i).locator("h5 a")
                product_name = link_tag.inner_text().strip()
                product_url_absolute = urljoin(BASE_URL, link_tag.get_attribute("href"))
                product_id = parse_qs(urlparse(product_url_absolute).query).get("ProductId", [None])[0]

                # --- FÄ°YAT ALMA BÃ–LÃœMÃœ GÃœNCELLENDÄ° ---
                price_value = 0.0
                currency = None # YENÄ°: Currency iÃ§in deÄŸiÅŸken
                
                price_tag = product_divs.nth(i).locator("strong.fiyatTooltip.pointer")
                if price_tag.count() > 0:
                    # SayÄ±sal deÄŸeri al (Mevcut mantÄ±k)
                    data_price = price_tag.first.get_attribute("data-price")
                    if data_price:
                        price_value = float(data_price.replace(",", "."))
                    
                    # YENÄ°: Para birimini al
                    currency = price_tag.first.get_attribute("data-currency")

                stock_divs = product_divs.nth(i).locator("span.stock-status")
                stock_info = " | ".join([
                    stock_divs.nth(j).inner_text().strip() for j in range(stock_divs.count())
                ]) if stock_divs.count() > 0 else "BelirtilmemiÅŸ"

                # DEÄÄ°ÅTÄ°: Append edilen sÃ¶zlÃ¼k gÃ¼ncellendi
                products_on_page.append({
                    "product_id": product_id,
                    "name": product_name,
                    "url": product_url_absolute,
                    "category_id": category_id,
                    "price": price_value,        # KarÅŸÄ±laÅŸtÄ±rma iÃ§in sayÄ±sal fiyat
                    "currency": currency,        # YENÄ°: Para birimi
                    "stock_info": stock_info,
                    "last_updated": time.strftime("%Y-%m-%d %H:%M:%S"),
                    # KALDIRILDI: price_display ve price_str alanlarÄ± silindi
                })
            except Exception as e:
                print(f"âš ï¸ ÃœrÃ¼n ayrÄ±ÅŸtÄ±rma hatasÄ± (Kategori {category_id}, Sayfa {page_num}): {e}")
        
        print(f"âœ… Kategori {category_id}, Sayfa {page_num}: {len(products_on_page)} Ã¼rÃ¼n bulundu.")
        all_products_in_category.extend(products_on_page)

        time.sleep(1)
    
    return all_products_in_category

# --- Ana Ã‡alÄ±ÅŸtÄ±rma Fonksiyonu ---
def run_scraper():
    print("ğŸš€ Bayinet Scraper baÅŸlÄ±yor...")

    with sync_playwright() as p:
        page = manual_login_and_get_session(p)

        print("âœ… Oturum hazÄ±r, scraping baÅŸlÄ±yor...")
        # Kategori ID'lerini 01'den 51'e (52 dahil deÄŸil) kadar Ã§eker
        for i in range(1, 52): 
            category_id = f"{i:02d}"
            print(f"\nğŸ“‚ Kategori {category_id} Ã§ekiliyor...")
            
            # 1. AdÄ±m: O kategorideki TÃœM Ã¼rÃ¼nleri Ã§ek
            products_from_category = scrape_all_pages(page, category_id)
            
            # 2. AdÄ±m: Ã‡ekilen Ã¼rÃ¼nleri, last_price mantÄ±ÄŸÄ±nÄ± iÃ§eren fonksiyona gÃ¶nder
            if products_from_category:
                print(f"ğŸ’¾ {category_id} kategorisinden {len(products_from_category)} Ã¼rÃ¼n DB'ye yazÄ±lmak Ã¼zere gÃ¶nderiliyor...")
                save_products_to_supabase(products_from_category)
            else:
                print(f"â„¹ï¸ {category_id} kategorisinden hiÃ§ Ã¼rÃ¼n Ã§ekilemedi.")

        print("\nâœ… Scraping tamamlandÄ±.")

# --- Script'in BaÅŸlangÄ±Ã§ NoktasÄ± ---
if __name__ == "__main__":
    run_scraper()