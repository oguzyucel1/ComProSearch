import os
import re
import time
import shutil
import pandas as pd
from urllib.parse import urljoin
from dotenv import load_dotenv
from playwright.sync_api import sync_playwright
from scripts.shared.supabase_client import supabase

load_dotenv()

EMAIL = os.getenv("DENGE_EMAIL")
PASSWORD = os.getenv("DENGE_PASSWORD")

if not EMAIL or not PASSWORD:
    raise RuntimeError("ğŸš¨ .env dosyasÄ±nda DENGE_EMAIL ve DENGE_PASSWORD tanÄ±mlÄ± olmalÄ±!")

BASE_URL = "https://www.edenge.com.tr"
DOWNLOAD_DIR = "downloads"

# --- Supabase kayÄ±t ---
def save_products_to_supabase(products, batch_size=50):
    if not products or not supabase:
        print("âŒ Supabase client eksik veya Ã¼rÃ¼n listesi boÅŸ. KayÄ±t atlandÄ±.")
        return

    for i in range(0, len(products), batch_size):
        chunk = products[i:i+batch_size]
        for attempt in range(3):
            try:
                data = (
                    supabase.table("denge_products")
                    .upsert(chunk, on_conflict="product_id")
                    .execute()
                )
                print(f"âœ… DB'ye {len(data.data)} Ã¼rÃ¼n yazÄ±ldÄ± (chunk {i//batch_size+1})")
                break
            except Exception as e:
                print(f"âš ï¸ Supabase error (chunk {i//batch_size+1}), retry {attempt+1}/3: {e}")
                time.sleep(5)

# --- Excel parse ---
def parse_excel(file_path, category_name):
    try:
        dfs = pd.read_html(file_path, header=2)
        df = dfs[0]
        print(f"âœ… {file_path} HTML tablodan okundu")
    except Exception as e:
        print(f"âŒ Tablo okunamadÄ±: {file_path}, hata: {e}")
        return []

    df = df[["AÃ§Ä±klama", "Ã–zel Fiyat", "Sk Fiyat", "PB", "Stok"]].copy()

    def clean_price(x):
        if pd.isna(x):
            return None
        s = str(x).replace(".", "").replace(",", ".")
        s = "".join(ch for ch in s if (ch.isdigit() or ch == "."))
        try:
            return float(s)
        except:
            return None

    df["Ã–zel Fiyat"] = df["Ã–zel Fiyat"].apply(clean_price)
    df["Sk Fiyat"] = df["Sk Fiyat"].apply(clean_price)

    products = []
    for _, row in df.iterrows():
        name = str(row["AÃ§Ä±klama"]) if pd.notna(row["AÃ§Ä±klama"]) else "Unknown"
        product_id = re.sub(r"[^a-zA-Z0-9]", "_", name)[:50]

        products.append({
            "product_id": product_id,
            "name": name,
            "special_price": row["Ã–zel Fiyat"],
            "list_price": row["Sk Fiyat"],
            "currency": row["PB"],
            "stock_info": row["Stok"],
            "category": category_name,
            "marketplace": "edenge",
            "last_updated": time.strftime("%Y-%m-%d %H:%M:%S")
        })
    return products


# --- Login + OTP ---
def eden_login(p):
    browser = p.chromium.launch(headless=False, slow_mo=80)
    context = browser.new_context(accept_downloads=True)
    page = context.new_page()

    page.goto(f"{BASE_URL}/Account/Login")
    page.wait_for_selector("#username_", timeout=10000)

    # Email & ÅŸifre doldur
    page.fill("#username_", EMAIL)
    page.fill("#password_", PASSWORD)
    print("âœ… E-Posta ve Åifre dolduruldu")

    # GiriÅŸ yap
    page.click("button[type='submit']")
    print("ğŸš€ GiriÅŸ butonuna tÄ±klandÄ±, OTP ekranÄ± bekleniyor...")

        # OTP inputunu bekle
    page.wait_for_selector("#smscode", timeout=60000)

    otp_code = input("ğŸ”‘ OTP kodunu gir (6 hane): ").strip()
    page.fill("#smscode", otp_code)
    print("âœ… OTP kodu dolduruldu")

    # ğŸ”¹ Radio label Ã¼zerinden tÄ±kla (Bu TarayÄ±cÄ±ya GÃ¼venme)
    try:
        page.click("label:has-text('Bu TarayÄ±cÄ±ya GÃ¼venme')", timeout=5000)
        print("â˜‘ï¸ 'Bu TarayÄ±cÄ±ya GÃ¼venme' seÃ§ildi (label click)")
    except Exception:
        # fallback: checked = true
        radio = page.query_selector("input[name='IsTrusted'][value='false']")
        if radio:
            page.evaluate("el => el.checked = true", radio)
            print("â˜‘ï¸ 'Bu TarayÄ±cÄ±ya GÃ¼venme' force seÃ§ildi (JS)")

    # ğŸ”¹ Devam et â†’ Ã¶nce normal click, sonra fallback submit
    try:
        page.wait_for_selector("button.button-1[type='submit']", timeout=5000)
        page.click("button.button-1[type='submit']", force=True)
        print("â–¶ï¸ Devam et tÄ±klandÄ±")
        # kÃ¼Ã§Ã¼k bekleme, Ã§alÄ±ÅŸmazsa fallback Ã§alÄ±ÅŸacak
        page.wait_for_timeout(2000)
    except Exception:
        print("âš ï¸ Devam Et click olmadÄ±, fallback'e geÃ§iliyor")

    # ğŸ”¹ Fallback: formu submit et
    try:
        page.evaluate("document.querySelector('form#js_submit').submit()")
        print("â–¶ï¸ Form JS ile submit edildi (fallback)")
    except Exception as e:
        print("âŒ Form submit edilemedi:", e)

    # Ana sayfa bekle
    page.wait_for_selector("a.navigation-categories-item-title", timeout=60000)
    print("ğŸ Ana sayfa yÃ¼klendi:", page.url)


    return page, browser


# --- Main scraper ---
def run_scraper():
    os.makedirs(DOWNLOAD_DIR, exist_ok=True)
    print("ğŸš€ Edenge Scraper baÅŸlÄ±yor...")

    with sync_playwright() as p:
        page, browser = eden_login(p)

        # Kategori linkleri
        category_elems = page.query_selector_all("a.navigation-categories-item-title")
        categories = []
        for elem in category_elems:
            href = elem.get_attribute("href")
            name = elem.inner_text().strip()
            if href:
                categories.append((name, urljoin(BASE_URL, href)))
        print(f"ğŸ” {len(categories)} kategori bulundu")

        # Kategori bazlÄ± scraping
        for i, (cat_name, cat_url) in enumerate(categories, start=1):
            print(f"\nâ¡ï¸ {i}. Kategori: {cat_name}")

            page.goto(cat_url)
            page.wait_for_load_state("networkidle")

            excel_btn = page.query_selector("a.js_exportexcel")
            if not excel_btn:
                print("âŒ Excel butonu bulunamadÄ±")
                continue

            with page.expect_download() as dl_info:
                excel_btn.click()
            download = dl_info.value

            safe_name = re.sub(r"[\\/:\*\?\"<>\|]", "_", cat_name)[:80]
            file_path = os.path.join(DOWNLOAD_DIR, f"{i}_{safe_name}.xls")
            download.save_as(file_path)
            print(f"â¬‡ï¸ Excel indirildi: {file_path}")

            products = parse_excel(file_path, cat_name)
            save_products_to_supabase(products)

            page.goto(BASE_URL)

        browser.close()

    # Cleanup
    try:
        shutil.rmtree(DOWNLOAD_DIR)
        print("ğŸ—‘ï¸ Ä°ndirilen Excel dosyalarÄ± silindi.")
    except Exception as e:
        print(f"âš ï¸ KlasÃ¶r silinemedi: {e}")


if __name__ == "__main__":
    run_scraper()
